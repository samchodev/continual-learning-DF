{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model \n",
    "from Model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb):   \n",
    "    loss = 0\n",
    "    current = model.trainable_weights \n",
    "    \n",
    "    for F, c, o in zip(fisher_matrix, current, optimal_weights):\n",
    "        loss += tf.reduce_sum(F * ((c - o) ** 2))\n",
    "\n",
    "\n",
    "    return loss * (lamb / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(model, fisher_matrix, lamb):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = CategoricalCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb=lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix(model, data, num_sample=10):\n",
    "\n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(tensor) for tensor in weights]\n",
    "\n",
    "    # num_sample 개의 데이터 랜덤샘플링 \n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "\n",
    "    for i in indices:\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            output = model(x, training=False) # (수정) 메모리 문제, 모든 데이터를 한번에 넣으면 오류 생김. 여기서는 하나씩 열개의 데이터를 사용 \n",
    "            log_likelihood = tf.math.log(output)\n",
    "\n",
    "        gradients = tape.gradient(log_likelihood, weights)\n",
    "        variance = [var + (grad ** 2) for var, grad in zip(variance, gradients)]\n",
    "\n",
    "    fisher_matrix = [tensor / num_sample for tensor in variance]\n",
    "    \n",
    "    return fisher_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (수정) 배치를 사용하지 않음 \n",
    "def evaluate(model, test_set):\n",
    "  acc = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "  for i, (seq, labels) in enumerate(test_set):\n",
    "    preds = model.predict_on_batch(seq)\n",
    "    acc.update_state(labels, preds)\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_joint(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                inc_train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "                train = accumulate_data(train, inc_train)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ce ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss_ce(model, fisher_matrix, lamb, alpha):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = alpha * CategoricalCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb=lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_ce(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, alpha=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb, alpha=alpha), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('mon_data.pkl')\n",
    "print(data.shape)\n",
    "MAX_LABEL = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop_joint(model, OPTIMIZER, data, test_size=0.2, first_task = 39, inc_task = 10, first_epochs = 1, inc_epochs = 1, lamb=0.0, num_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 빌드 \n",
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop(model, OPTIMIZER, data, test_size=0.2, first_task = 39, inc_task = 10, first_epochs = 100, inc_epochs = 50, lamb=0.0, num_sample=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naivis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
